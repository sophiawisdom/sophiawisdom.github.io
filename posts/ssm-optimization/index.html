<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Ssm Optimization | Sophia Wisdom's Blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="I didn&rsquo;t have a job and was jonesing for some fun. A friend was working on State Space Models. What better use of my time could there be than being her performance engineer, and learning GPU internals while I was at it? We got cracking at Noisebridge, and in the course of a few amphetamine-fueled hours of writing and thinking, we had a should-be-functional Triton kernel. Unfortunately our code was so perfect we crashed the compiler in two different ways."><meta name=generator content="Hugo 0.109.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="Ssm Optimization"><meta property="og:description" content="I didn&rsquo;t have a job and was jonesing for some fun. A friend was working on State Space Models. What better use of my time could there be than being her performance engineer, and learning GPU internals while I was at it? We got cracking at Noisebridge, and in the course of a few amphetamine-fueled hours of writing and thinking, we had a should-be-functional Triton kernel. Unfortunately our code was so perfect we crashed the compiler in two different ways."><meta property="og:type" content="article"><meta property="og:url" content="https://sophiawisdom.github.io/posts/ssm-optimization/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-17T04:20:46-08:00"><meta property="article:modified_time" content="2023-01-17T04:20:46-08:00"><meta itemprop=name content="Ssm Optimization"><meta itemprop=description content="I didn&rsquo;t have a job and was jonesing for some fun. A friend was working on State Space Models. What better use of my time could there be than being her performance engineer, and learning GPU internals while I was at it? We got cracking at Noisebridge, and in the course of a few amphetamine-fueled hours of writing and thinking, we had a should-be-functional Triton kernel. Unfortunately our code was so perfect we crashed the compiler in two different ways."><meta itemprop=datePublished content="2023-01-17T04:20:46-08:00"><meta itemprop=dateModified content="2023-01-17T04:20:46-08:00"><meta itemprop=wordCount content="1311"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Ssm Optimization"><meta name=twitter:description content="I didn&rsquo;t have a job and was jonesing for some fun. A friend was working on State Space Models. What better use of my time could there be than being her performance engineer, and learning GPU internals while I was at it? We got cracking at Noisebridge, and in the course of a few amphetamine-fueled hours of writing and thinking, we had a should-be-functional Triton kernel. Unfortunately our code was so perfect we crashed the compiler in two different ways."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Sophia Wisdom's Blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Ssm Optimization</h1><time class="f6 mv4 dib tracked" datetime=2023-01-17T04:20:46-08:00>January 17, 2023</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><a href=https://twitter.com/cis_female>I</a> didn&rsquo;t have a job and was jonesing for some fun. <a href=https://twitter.com/typedfemale>A friend</a> was working on State Space Models. What better use of my time could there be than being her performance engineer, and learning GPU internals while I was at it? We got cracking at <a href=https://noisebridge.net/>Noisebridge</a>, and in the course of a few amphetamine-fueled hours of writing and thinking, we had a should-be-functional <a href=https://github.com/openai/triton>Triton</a> kernel. Unfortunately our code was so perfect we crashed the compiler in <a href=https://github.com/openai/triton/issues/639>two</a> <a href=https://github.com/openai/triton/issues/640>different</a> ways. Still amphetamine-fueled and now disgusted with Triton and determined to Go Deeper, I went home and read the entire <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html>PTX ISA documentation</a>. The next day I started working on a PTX kernel for Multi-Input Multi-Output (&ldquo;MIMO&rdquo;) state space models. It took two weeks of scribbling about in-register transposes on a notebook and was deemed &ldquo;<a href=https://twitter.com/typedfemale/status/1571025861155127296>psychotic</a>&rdquo; by fans. Unfortunately it was worth jack shit. The theory behind MIMO was that GPUs are much better at matrix math than vector math, so if we transfigured our problem from vector to matrix we could make more efficient use of the <a href=https://www.nvidia.com/en-us/data-center/tensor-cores/>underlying hardware</a>. However, this transfiguration introduced so much overhead it wasn&rsquo;t faster then the direct calculation. We&rsquo;ll talk more about this later, but just remember for now that I was able to get about 22m elems/s @ STATE_SIZE=64.
// TODO: not properly contextualizing the 50k/s of torch</p><p>Later the friend started to get into a different way to train State Space Models where you diagonalize the A matrix. This means that instead of doing <code>STATE_SIZE*2</code> flops for B+C and<code>STATE_SIZE^2</code> flops for A, we now do a total of <code>STATE_SIZE*3</code> flops, for A+B+C combined. This means that flops-wise we&rsquo;re no longer totally bottlenecked on the flops of A. There are a lot of questions about how numerically stable this is and whether it can really learn. But it&rsquo;s a fun challenge for optimization, because the math operation we&rsquo;re doing is now quite simple: For each head, take in three vectors: A, B, C, all of the same time. At every timestep, multiply your state by A, load in an input, multiply the input by B, add the state and the input together, multiply the resulting state by C, then write out the output. However, by default this is implemented in a horrendously inefficient way.</p><h2 id=first-implementation-in-torch>First implementation in Torch</h2><p>Here&rsquo;s a simple Pytorch function that implements this operation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@torch.jit.script</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>torch_diag</span>(sequence, A, B, C):
</span></span><span style=display:flex><span>    <span style=color:#75715e># get our shapes from tensors passed in</span>
</span></span><span style=display:flex><span>    N_HEADS, SEQUENCE_LENGTH <span style=color:#f92672>=</span> sequence<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>    STATE_SIZE <span style=color:#f92672>=</span> B<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># allocate our output</span>
</span></span><span style=display:flex><span>    torch_diag_outputs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>empty((N_HEADS, SEQUENCE_LENGTH), dtype<span style=color:#f92672>=</span>sequence<span style=color:#f92672>.</span>dtype, device<span style=color:#f92672>=</span>sequence<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(N_HEADS):
</span></span><span style=display:flex><span>        <span style=color:#75715e># create our state</span>
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((STATE_SIZE,), dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float32, device<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(SEQUENCE_LENGTH):
</span></span><span style=display:flex><span>            <span style=color:#75715e># multiply previous state by A</span>
</span></span><span style=display:flex><span>            previous_state_multed <span style=color:#f92672>=</span> A[i] <span style=color:#f92672>*</span> state
</span></span><span style=display:flex><span>            <span style=color:#75715e># multiply new input by B</span>
</span></span><span style=display:flex><span>            input_by_b <span style=color:#f92672>=</span> B[i] <span style=color:#f92672>*</span> sequence[i][j]
</span></span><span style=display:flex><span>            <span style=color:#75715e># add them together to get new state</span>
</span></span><span style=display:flex><span>            state <span style=color:#f92672>=</span> previous_state_multed <span style=color:#f92672>+</span> input_by_b
</span></span><span style=display:flex><span>            <span style=color:#75715e># multiply new state by C</span>
</span></span><span style=display:flex><span>            state_by_c <span style=color:#f92672>=</span> C[i] <span style=color:#f92672>*</span> state
</span></span><span style=display:flex><span>            <span style=color:#75715e># sum the result</span>
</span></span><span style=display:flex><span>            summed <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sum(state_by_c)
</span></span><span style=display:flex><span>            <span style=color:#75715e># write to output buffer</span>
</span></span><span style=display:flex><span>            torch_diag_outputs[(i, j)] <span style=color:#f92672>=</span> summed
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch_diag_outputs
</span></span></code></pre></div><p>This function can do approximately 50,000 elements/second on an A100 (all perf calculations are on A100s). An &ldquo;element&rdquo; in my calculation is one value for one head, so e.g. 32 heads with 512 sequence length each is 16,384 &ldquo;elements&rdquo;. I calculate this with <code>STATE_SIZE=32</code>. This is absolutely abysmal performance from Pytorch. Why? It launches a separate kernel and waits for it to complete for every operation, where we do only <code>STATE_SIZE</code> flops. This means that almost all of the power of the GPU is wasted on overhead. I never thought pure Torch would be performant, though, so I didn&rsquo;t spend much time on this and moved quickly to Triton.</p><h2 id=triton>Triton</h2><p>I have incredibly mixed feelings about Triton. When it works, it&rsquo;s magical. You write Torch-like vector/matrix operations and with only academic knowledge of the GPU even Research Scientists can produce highly performant code. The struggle of Triton is the compiler. Every time I have tried to write Triton code I&rsquo;ve spent 1 hour writing the kernel and 10 hours debugging segfaults in the compiler. Even the <a href=https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py#L18>language tutorials</a> have comments describing how they got around compiler bugs. The raison d&rsquo;être for Triton&rsquo;s existence, in my opinion, is efficient code generation with matmuls, because matmuls and tensor cores break the fundamental abstraction of CUDA, the thread. Even though we&rsquo;re not using matmuls here, Triton is still quite good. Let&rsquo;s look at the code I wrote for the ssm_kernel:</p><p>TODO: my triton code is wrong somehow, need to fix it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@triton.jit</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ssm_kernel</span>(sequence_ptr, a_ptr, b_ptr, c_ptr, output_ptr,
</span></span><span style=display:flex><span>               SEQUENCE_LENGTH: tl<span style=color:#f92672>.</span>constexpr, STATE_SIZE: tl<span style=color:#f92672>.</span>constexpr,
</span></span><span style=display:flex><span>               N_HEADS: tl<span style=color:#f92672>.</span>constexpr):
</span></span><span style=display:flex><span>    i <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>program_id(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#75715e># which head we&#39;re on</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># load vectors. they will be held in registers</span>
</span></span><span style=display:flex><span>    A <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(a_ptr <span style=color:#f92672>+</span> i <span style=color:#f92672>*</span> STATE_SIZE <span style=color:#f92672>+</span> tl<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, STATE_SIZE))
</span></span><span style=display:flex><span>    B <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(b_ptr <span style=color:#f92672>+</span> i <span style=color:#f92672>*</span> STATE_SIZE <span style=color:#f92672>+</span> tl<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, STATE_SIZE))
</span></span><span style=display:flex><span>    C <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(c_ptr <span style=color:#f92672>+</span> i <span style=color:#f92672>*</span> STATE_SIZE <span style=color:#f92672>+</span> tl<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, STATE_SIZE))
</span></span><span style=display:flex><span>    <span style=color:#75715e># create our state</span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>zeros((STATE_SIZE,), dtype<span style=color:#f92672>=</span>tl<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(SEQUENCE_LENGTH):
</span></span><span style=display:flex><span>	    <span style=color:#75715e># load input value</span>
</span></span><span style=display:flex><span>        input_value <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(sequence_ptr<span style=color:#f92672>+</span> j <span style=color:#f92672>*</span> N_HEADS)
</span></span><span style=display:flex><span>        <span style=color:#75715e># multiply previous state by A, new input by B, add them together</span>
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> state <span style=color:#f92672>*</span> A <span style=color:#f92672>+</span> B <span style=color:#f92672>*</span> input_value
</span></span><span style=display:flex><span>        <span style=color:#75715e># multiply new state by C and sum result</span>
</span></span><span style=display:flex><span>        output_value <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>sum(state<span style=color:#f92672>*</span>C, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># write to output buffer</span>
</span></span><span style=display:flex><span>        tl<span style=color:#f92672>.</span>store(output_ptr <span style=color:#f92672>+</span> (j <span style=color:#f92672>*</span> N_HEADS <span style=color:#f92672>+</span> i), output_value)
</span></span></code></pre></div><p>This looks similar to Torch, but with the added complexity of thinking directly about memory loads and stores as opposed to just indexing into a matrix. However, it blows Torch out of the water on speed: !!4,600,000,000!! elements/s. Why is this? For one thing, it sends just one massive operation to the GPU instead of <code>5 * SEQUENCE_LENGTH * N_HEADS</code> operations. How do we understand what&rsquo;s actually going on inside this thing though? Just look at <a href=https://godbolt.org/z/4dT54Ejhd>the assembly</a>!</p><p>First a quick digression &ndash; what is assembly on GPUs? By &ldquo;assembly&rdquo; here I mean <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html>PTX</a> which is a language NVIDIA created as a lower level than CUDA so others writing high-level languages (e.g. <a href=https://research.google/pubs/pub45226/>Google&rsquo;s CUDA compiler</a>, <a href=https://github.com/halide/Halide>Halide</a>, Triton) could have a low-level language to compile to. If you want to understand what Triton is actually doing, you have to look at the PTX. So off we go.</p><p>The inner loop looks something like this:</p><pre tabindex=0><code>LBB0_1:
	ld.global.b32 {%r13}, [ %rd21 + 0]; // load next input to %r13
	mov.b32  %f17, %r13; // move the value in %r13 (generic 32 bit register) to %f17 (floating point register).
	// this does nothing physically.
	mul.f32  %f18, %f2, %f17; // B (%f2) * input (%f17) -&gt; %f18
	fma.rn.f32  %f20, %f20, %f1, %f18; // A (%f1) * X (%f20) + f18 (B * input) -&gt; X (%f20)

	// calculate pointer to write to. Why does it do this in the inner loop?
	// Your guess is as good as mine.
	mul.wide.s32  %rd20, %r20, 4; // multiply head we&#39;re in by 4 and put it in %rd20
	add.s64  %rd19, %rd4, %rd20; // add %rd20 (head * 4) to output ptr

	mul.f32  %f8, %f20, %f3; // X (%f20) * C (%f3) -&gt; %f8

	// Five butterfly shuffles. Butterfly shuffles need a diagram to explain, so see the main text.
	shfl.sync.bfly.b32  %f7, %f8, 16, 0x1f, 0xffffffff;
	fma.rn.f32  %f10, %f20, %f3, %f7; // here it recalculates %f8 (%f20 * %f3) for some reason
	shfl.sync.bfly.b32  %f9, %f10, 8, 0x1f, 0xffffffff;
	add.f32  %f12, %f10, %f9;
	shfl.sync.bfly.b32  %f11, %f12, 4, 0x1f, 0xffffffff;
	add.f32  %f14, %f12, %f11;
	shfl.sync.bfly.b32  %f13, %f14, 2, 0x1f, 0xffffffff;
	add.f32  %f16, %f14, %f13;
	shfl.sync.bfly.b32  %f15, %f16, 1, 0x1f, 0xffffffff;
	add.f32  %f19, %f16, %f15;
	mov.b32  %r19, %f19; // completed calculating butterfly shuffles

	st.global.b32 [ %rd19 + 0] , {%r19}; // write out output

	add.s32  %r21, %r21, 1; // increment sequence index
	add.s32  %r20, %r20, 16; // add 16 to the output ptr. ??? why 16
	add.s64  %rd21, %rd21, 64; // increment u_ptr by 16
	setp.lt.s32  %p6, %r21, 8192;
	@%p6 bra LBB0_1;
	ret;
</code></pre><p>crappy butterfly shuffle diagram, will make my own.
<img src=https://www.researchgate.net/publication/317485271/figure/fig1/AS:505251083100160@1497472653903/Data-exchange-among-8-threads-using-butterfly-warp-shuffle-operations.png alt="Data exchange among 8 threads using butterfly warp shuffle operations. |  Download Scientific Diagram"></p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://sophiawisdom.github.io/>&copy; Sophia Wisdom's Blog 2023</a><div><div class=ananke-socials></div></div></div></footer></body></html>