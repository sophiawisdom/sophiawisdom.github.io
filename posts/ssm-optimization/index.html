<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel | Sophia Wisdom's Blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="I didn&rsquo;t have a job and was jonesing for some fun. A friend was working on a new machine learning architecture called State Space Models (SSMs). What better use of my time than being her personal performance engineer? We got cracking at Noisebridge, and in the course of a few amphetamine-fueled hours, we had a should-be-functional Triton kernel. Unfortunately our code was so perfect we crashed the compiler in two different ways."><meta name=generator content="Hugo 0.110.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel"><meta property="og:description" content="I didn&rsquo;t have a job and was jonesing for some fun. A friend was working on a new machine learning architecture called State Space Models (SSMs). What better use of my time than being her personal performance engineer? We got cracking at Noisebridge, and in the course of a few amphetamine-fueled hours, we had a should-be-functional Triton kernel. Unfortunately our code was so perfect we crashed the compiler in two different ways."><meta property="og:type" content="article"><meta property="og:url" content="https://sophiawisdom.github.io/posts/ssm-optimization/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-17T04:20:46-08:00"><meta property="article:modified_time" content="2023-01-17T04:20:46-08:00"><meta itemprop=name content="SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel"><meta itemprop=description content="I didn&rsquo;t have a job and was jonesing for some fun. A friend was working on a new machine learning architecture called State Space Models (SSMs). What better use of my time than being her personal performance engineer? We got cracking at Noisebridge, and in the course of a few amphetamine-fueled hours, we had a should-be-functional Triton kernel. Unfortunately our code was so perfect we crashed the compiler in two different ways."><meta itemprop=datePublished content="2023-01-17T04:20:46-08:00"><meta itemprop=dateModified content="2023-01-17T04:20:46-08:00"><meta itemprop=wordCount content="1846"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel"><meta name=twitter:description content="I didn&rsquo;t have a job and was jonesing for some fun. A friend was working on a new machine learning architecture called State Space Models (SSMs). What better use of my time than being her personal performance engineer? We got cracking at Noisebridge, and in the course of a few amphetamine-fueled hours, we had a should-be-functional Triton kernel. Unfortunately our code was so perfect we crashed the compiler in two different ways."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Sophia Wisdom's Blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel</h1><time class="f6 mv4 dib tracked" datetime=2023-01-17T04:20:46-08:00>January 17, 2023</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><a href=https://twitter.com/cis_female>I</a> didn&rsquo;t have a job and was jonesing for some fun. <a href=https://twitter.com/typedfemale>A friend</a> was working on a new machine learning architecture called State Space Models (SSMs). What better use of my time than being her personal performance engineer? We got cracking at <a href=https://noisebridge.net/>Noisebridge</a>, and in the course of a few amphetamine-fueled hours, we had a should-be-functional <a href=https://github.com/openai/triton>Triton</a> kernel. Unfortunately our code was so perfect we crashed the compiler in <a href=https://github.com/openai/triton/issues/639>two</a> <a href=https://github.com/openai/triton/issues/640>different</a> ways. Still amphetamine-fueled but now disgusted with Triton and determined to Go Deeper, I went home and read the entire <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html>PTX ISA documentation</a>. The next day I started working on a PTX kernel for MIMO state space models. It took two weeks of scribbling about in-register transposes on a notebook and was deemed &ldquo;<a href=https://twitter.com/typedfemale/status/1571025861155127296>psychotic</a>&rdquo; by fans. But it was useless &ndash; my scribbles went in the trash, my psychotic creation crushed by reality&rsquo;s Risperdal.</p><p>I personally tend to eschew intuitive explanations of architectures and am instead attracted to more <a href=https://blog.nelhage.com/post/transformers-for-software-engineers/>mechanistic explanations</a> of what the underlying math operation is. For State Space Models, the math is that you have some single-dimensional input array <code>u</code> of length <code>SEQUENCE_LENGTH</code> and a single-dimensional output array <code>y</code> of the same size. At every time step, you have a state vector, X, of size <code>STATE_SIZE</code>. A typical size for <code>STATE_SIZE</code> is e.g. 32 or 64. You have three parameters you try to fit: 1) a state transition matrix <code>A</code> of size (<code>STATE_SIZE</code>, <code>STATE_SIZE</code>), 2) a &ldquo;new information introduction vector&rdquo; <code>B</code> of size <code>STATE_SIZE</code>, and 3) a &ldquo;state to output&rdquo; vector <code>C</code> also of size <code>STATE_SIZE</code>. You then use these parameters as such to calculate your outputs: Xₜ = AXₜ₋₁ + Buₜ, yₜ=CXₜ . Note that this is totally linear. The vast majority of the work in these systems comes from multiplying A by X, because those are much bigger . Dealing with this has been <em>the</em> major difficulty in making SSMs work practically.</p><p>A few months after the crushing defeat of MIMO, the <a href=https://en.wikipedia.org/wiki/Public_Universal_Friend>Friend</a> started to get into a different way to train SSMs where you <a href=https://mathworld.wolfram.com/MatrixDiagonalization.html>diagonalize</a> the A matrix. This reduces the computational requirements dramatically, from O(STATE_SIZE^2) flops to O(STATE_SIZE) flops <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. There are a lot of questions about how numerically stable this is and whether it can really learn. But it&rsquo;s a fun challenge for optimization, because the math operation we&rsquo;re doing is now quite simple: For each head, take in three vectors: A, B, C, all of the same time. At every timestep, multiply your state by A, load in an input, multiply the input by B, add the state and the input together, multiply the resulting state by C, then write out the output.</p><p>So how fast can we make it? How long must we think to achieve optimal performance?</p><h2 id=torch>Torch</h2><p>A natural first choice to implement any machine learning algorithm is Pytorch. Here&rsquo;s a simple Pytorch function that implements our mathematical operation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@torch.jit.script</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>torch_diag</span>(sequence, A, B, C):
</span></span><span style=display:flex><span>    <span style=color:#75715e># get our shapes from tensors passed in</span>
</span></span><span style=display:flex><span>    N_HEADS, SEQUENCE_LENGTH <span style=color:#f92672>=</span> sequence<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>    STATE_SIZE <span style=color:#f92672>=</span> B<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#75715e># allocate our output</span>
</span></span><span style=display:flex><span>    torch_diag_outputs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>empty((N_HEADS, SEQUENCE_LENGTH), dtype<span style=color:#f92672>=</span>sequence<span style=color:#f92672>.</span>dtype, device<span style=color:#f92672>=</span>sequence<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(N_HEADS):
</span></span><span style=display:flex><span>        <span style=color:#75715e># create our state</span>
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((STATE_SIZE,), dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float32, device<span style=color:#f92672>=</span>sequence<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(SEQUENCE_LENGTH):
</span></span><span style=display:flex><span>            previous_state_multed <span style=color:#f92672>=</span> A[i] <span style=color:#f92672>*</span> state <span style=color:#75715e># multiply previous state by A</span>
</span></span><span style=display:flex><span>            input_by_b <span style=color:#f92672>=</span> B[i] <span style=color:#f92672>*</span> sequence[i][j] <span style=color:#75715e># multiply new input by B</span>
</span></span><span style=display:flex><span>            state <span style=color:#f92672>=</span> previous_state_multed <span style=color:#f92672>+</span> input_by_b <span style=color:#75715e># add them together to get new state</span>
</span></span><span style=display:flex><span>            state_by_c <span style=color:#f92672>=</span> C[i] <span style=color:#f92672>*</span> state <span style=color:#75715e># multiply new state by C</span>
</span></span><span style=display:flex><span>            summed <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sum(state_by_c) <span style=color:#75715e># sum the result</span>
</span></span><span style=display:flex><span>            torch_diag_outputs[(i, j)] <span style=color:#f92672>=</span> summed <span style=color:#75715e># write to output buffer</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch_diag_outputs
</span></span></code></pre></div><p>Fairly intuitive &ndash; so how does it perform?</p><p>Fucking awful! This function can process 50,000 elements<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>/second <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, which is slower than even a single CPU core. Why?
It launches a separate kernel and waits for it to complete for every operation, where it does only <code>STATE_SIZE</code> (32) flops. This means that almost all of the power of the GPU is wasted on overhead. I never thought pure Torch would be performant <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, though, so I didn&rsquo;t spend much time on this and moved quickly to Triton <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>.</p><h2 id=triton>Triton</h2><p>I have incredibly mixed feelings about Triton. When it works, it&rsquo;s magical. You write Torch-like vector/matrix operations and with only academic knowledge of the GPU even Research Scientists can produce highly performant code. The struggle of Triton is the compiler. Every other time I have tried to write Triton code I&rsquo;ve spent 1 hour writing the kernel and 10 hours debugging segfaults in the compiler. Even the <a href=https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py#L18>language tutorials</a> have comments describing how they got around compiler bugs. The raison d&rsquo;être for Triton&rsquo;s existence, in my opinion, is efficient code generation with matmuls, because matmuls and tensor cores break the fundamental abstraction of CUDA, the thread. Even though we&rsquo;re not using matmuls here, Triton is still quite good. Let&rsquo;s look at the code I wrote for the ssm_kernel:</p><p>// TODO: my triton code is wrong somehow, need to fix it. Need to make sure it produces correct values.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@triton.jit</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ssm_kernel</span>(sequence_ptr, a_ptr, b_ptr, c_ptr, output_ptr,
</span></span><span style=display:flex><span>               SEQUENCE_LENGTH: tl<span style=color:#f92672>.</span>constexpr, STATE_SIZE: tl<span style=color:#f92672>.</span>constexpr,
</span></span><span style=display:flex><span>               N_HEADS: tl<span style=color:#f92672>.</span>constexpr):
</span></span><span style=display:flex><span>    head <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>program_id(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#75715e># which head we&#39;re on</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Load A, B, C vectors for this head. they will be held in registers. </span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># i * STATE_SIZE + tl.arange(0, STATE_SIZE) means &#34;load the STATE_SIZE values after index i * STATE_SIZE&#34;.</span>
</span></span><span style=display:flex><span>    A <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(a_ptr <span style=color:#f92672>+</span> head <span style=color:#f92672>*</span> STATE_SIZE <span style=color:#f92672>+</span> tl<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, STATE_SIZE))
</span></span><span style=display:flex><span>    B <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(b_ptr <span style=color:#f92672>+</span> head <span style=color:#f92672>*</span> STATE_SIZE <span style=color:#f92672>+</span> tl<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, STATE_SIZE))
</span></span><span style=display:flex><span>    C <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(c_ptr <span style=color:#f92672>+</span> head <span style=color:#f92672>*</span> STATE_SIZE <span style=color:#f92672>+</span> tl<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, STATE_SIZE))
</span></span><span style=display:flex><span>    <span style=color:#75715e># create our state vector. this will be held in registers as well.</span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>zeros((STATE_SIZE,), dtype<span style=color:#f92672>=</span>tl<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(SEQUENCE_LENGTH):
</span></span><span style=display:flex><span>        <span style=color:#75715e># calculate what index the `i`th value has has for head `head`</span>
</span></span><span style=display:flex><span>        idx <span style=color:#f92672>=</span> (head <span style=color:#f92672>*</span> SEQUENCE_LENGTH <span style=color:#f92672>+</span> i)
</span></span><span style=display:flex><span>	    <span style=color:#75715e># load input value</span>
</span></span><span style=display:flex><span>        input_value <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(sequence_ptr <span style=color:#f92672>+</span> idx)
</span></span><span style=display:flex><span>        <span style=color:#75715e># multiply previous state by A, new input by B, add them together</span>
</span></span><span style=display:flex><span>        state <span style=color:#f92672>=</span> state <span style=color:#f92672>*</span> A <span style=color:#f92672>+</span> B <span style=color:#f92672>*</span> input_value
</span></span><span style=display:flex><span>        <span style=color:#75715e># multiply new state by C and sum result</span>
</span></span><span style=display:flex><span>        output_value <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>sum(state<span style=color:#f92672>*</span>C, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># write to output buffer</span>
</span></span><span style=display:flex><span>        tl<span style=color:#f92672>.</span>store(output_ptr <span style=color:#f92672>+</span> idx, output_value)
</span></span></code></pre></div><p>This looks similar to Torch, but with the added complexity of thinking directly about memory loads and stores as opposed to just indexing into a matrix. However, it blows Torch out of the water on speed: !!4,600,000,000!! elements/s. Why is this? For one thing, it sends just one massive operation to the GPU instead of <code>5 * SEQUENCE_LENGTH * N_HEADS</code> operations. How do we understand what&rsquo;s actually going on inside this thing though? Just look at <a href=https://godbolt.org/z/4dT54Ejhd>the assembly</a>! Many are scared of assembly, but on GPUs we generally write orders of magnitude less code and spend orders of magnitude more money on running it, so diving into the assembly is often worthwhile. So be courageous, and dive a level deeper with me!</p><p>First a quick digression &ndash; what is assembly on GPUs? It&rsquo;s not x86 assembly! By &ldquo;assembly&rdquo; here I mean <a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html>PTX</a> which is a language NVIDIA created as a lower level than CUDA so others writing high-level languages (e.g. <a href=https://research.google/pubs/pub45226/>Google&rsquo;s CUDA compiler</a>, <a href=https://github.com/halide/Halide>Halide</a>, Triton) could have a low-level language to compile to. /* This is sort of a bizarre situation, like if Intel had created hardware to run C on and then created assembly after the fact. TODO: include this?? <em>/. /</em> Below PTX there&rsquo;s another language called &ldquo;SASS&rdquo; which is like GPU microcode &ndash; the hardware executes it directly and it changes from generation to generation. NVIDIA really doesn&rsquo;t want you to use this, because if you write SASS it won&rsquo;t work with new GPUs. But you can do it anyway :p. TODO: include this? */ If you want to understand what Triton is actually doing, you have to look at the PTX. So off we go.</p><p>The inner loop of our compiled kernel looks like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span>LBB0_1:
</span></span><span style=display:flex><span>	ld.global.b32 {<span style=color:#f92672>%</span>r13}, [ <span style=color:#f92672>%</span>rd21 <span style=color:#f92672>+</span> <span style=color:#ae81ff>0</span>]; <span style=color:#75715e>// load input value to %r13
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	mov.b32  <span style=color:#f92672>%</span>f17, <span style=color:#f92672>%</span>r13; <span style=color:#75715e>// move the value in %r13 (generic 32 bit register) to %f17 (floating point register).
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#75715e>// this does nothing physically.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># multiply previous state by A, new input by B, add them together
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	mul.f32  <span style=color:#f92672>%</span>f18, <span style=color:#f92672>%</span>f2, <span style=color:#f92672>%</span>f17; <span style=color:#75715e>// multiply new input (%f17) by B (%f2) -&gt; %f18
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	fma.rn.f32  <span style=color:#f92672>%</span>f20, <span style=color:#f92672>%</span>f20, <span style=color:#f92672>%</span>f1, <span style=color:#f92672>%</span>f18; <span style=color:#75715e>// Multiply previous state (%f20) by A (%f1) and then add this to (new_input * B) (%f18)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// calculate pointer to write to. Why does it do this in the inner loop?
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#75715e>// Your guess is as good as mine.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	mul.wide.s32  <span style=color:#f92672>%</span>rd20, <span style=color:#f92672>%</span>r20, <span style=color:#ae81ff>4</span>; <span style=color:#75715e>// multiply head we&#39;re in by 4 and put it in %rd20
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	add.s64  <span style=color:#f92672>%</span>rd19, <span style=color:#f92672>%</span>rd4, <span style=color:#f92672>%</span>rd20; <span style=color:#75715e>// add %rd20 (head * 4) to output ptr
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	mul.f32  <span style=color:#f92672>%</span>f8, <span style=color:#f92672>%</span>f20, <span style=color:#f92672>%</span>f3; <span style=color:#75715e>// X (%f20) * C (%f3) -&gt; %f8
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// Five butterfly shuffles. Butterfly shuffles need a diagram to explain, so see the main text.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	shfl.sync.bfly.b32  <span style=color:#f92672>%</span>f7, <span style=color:#f92672>%</span>f8, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>0x1f</span>, <span style=color:#ae81ff>0xffffffff</span>;
</span></span><span style=display:flex><span>	fma.rn.f32  <span style=color:#f92672>%</span>f10, <span style=color:#f92672>%</span>f20, <span style=color:#f92672>%</span>f3, <span style=color:#f92672>%</span>f7; <span style=color:#75715e>// here it recalculates %f8 (%f20 * %f3) for some reason
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	shfl.sync.bfly.b32  <span style=color:#f92672>%</span>f9, <span style=color:#f92672>%</span>f10, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>0x1f</span>, <span style=color:#ae81ff>0xffffffff</span>;
</span></span><span style=display:flex><span>	add.f32  <span style=color:#f92672>%</span>f12, <span style=color:#f92672>%</span>f10, <span style=color:#f92672>%</span>f9;
</span></span><span style=display:flex><span>	shfl.sync.bfly.b32  <span style=color:#f92672>%</span>f11, <span style=color:#f92672>%</span>f12, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>0x1f</span>, <span style=color:#ae81ff>0xffffffff</span>;
</span></span><span style=display:flex><span>	add.f32  <span style=color:#f92672>%</span>f14, <span style=color:#f92672>%</span>f12, <span style=color:#f92672>%</span>f11;
</span></span><span style=display:flex><span>	shfl.sync.bfly.b32  <span style=color:#f92672>%</span>f13, <span style=color:#f92672>%</span>f14, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0x1f</span>, <span style=color:#ae81ff>0xffffffff</span>;
</span></span><span style=display:flex><span>	add.f32  <span style=color:#f92672>%</span>f16, <span style=color:#f92672>%</span>f14, <span style=color:#f92672>%</span>f13;
</span></span><span style=display:flex><span>	shfl.sync.bfly.b32  <span style=color:#f92672>%</span>f15, <span style=color:#f92672>%</span>f16, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0x1f</span>, <span style=color:#ae81ff>0xffffffff</span>;
</span></span><span style=display:flex><span>	add.f32  <span style=color:#f92672>%</span>f19, <span style=color:#f92672>%</span>f16, <span style=color:#f92672>%</span>f15;
</span></span><span style=display:flex><span>	mov.b32  <span style=color:#f92672>%</span>r19, <span style=color:#f92672>%</span>f19; <span style=color:#75715e>// completed calculating butterfly shuffles
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	st.global.b32 [ <span style=color:#f92672>%</span>rd19 <span style=color:#f92672>+</span> <span style=color:#ae81ff>0</span>] , {<span style=color:#f92672>%</span>r19}; <span style=color:#75715e>// write out output
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>	add.s32  <span style=color:#f92672>%</span>r21, <span style=color:#f92672>%</span>r21, <span style=color:#ae81ff>1</span>; <span style=color:#75715e>// increment sequence index
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	add.s32  <span style=color:#f92672>%</span>r20, <span style=color:#f92672>%</span>r20, <span style=color:#ae81ff>16</span>; <span style=color:#75715e>// add 16 to the output ptr. ??? why 16
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	add.s64  <span style=color:#f92672>%</span>rd21, <span style=color:#f92672>%</span>rd21, <span style=color:#ae81ff>64</span>; <span style=color:#75715e>// increment u_ptr by 16
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	setp.lt.s32  <span style=color:#f92672>%</span>p6, <span style=color:#f92672>%</span>r21, <span style=color:#ae81ff>8192</span>;
</span></span><span style=display:flex><span>	<span style=color:#960050;background-color:#1e0010>@</span><span style=color:#f92672>%</span>p6 bra LBB0_1;
</span></span><span style=display:flex><span>	ret;
</span></span></code></pre></div><p>If we compare to our original code, we can clearly see each line of code corresponding to one or two lines of assembly. Great! I&rsquo;m starting to feel like PTX is a breeze! But then there&rsquo;s this big block of shuffles. Now we have to start thinking about &ldquo;warps&rdquo; and &ldquo;threads&rdquo; so get ready.</p><p>crappy butterfly shuffle diagram, will make my own.
<img src=https://www.researchgate.net/publication/317485271/figure/fig1/AS:505251083100160@1497472653903/Data-exchange-among-8-threads-using-butterfly-warp-shuffle-operations.png alt="Data exchange among 8 threads using butterfly warp shuffle operations. |  Download Scientific Diagram"></p><p>why is <a href=https://godbolt.org/z/KbGTxb4oh>SSM kernel with STATE_SIZE=4096</a> 60x slower than <a href=https://godbolt.org/z/KbGTxb4oh>SSM kernel with STATE_SIZE=2048</a>? challenge for the devoted reader!</p><p>Remember how I told you it was useful reading assembly? I was just stringing you along for my real trap &ndash; it&rsquo;s useful writing assembly! And that&rsquo;s how we&rsquo;ll get the remaining speedup.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Specifically from 2 * STATE_SIZE^2 + 4 * STATE_SIZE flops to 5 * STATE_SIZE flops. For STATE_SIZE=32 this is a 13x decrease (2176 flops to 160 flops)&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>An &ldquo;element&rdquo; in my calculation is one value for one head, so e.g. 32 heads with 512 sequence length each is 16,384 &ldquo;elements&rdquo;&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>All benchmarks are performed on an A100 and with <code>STATE_SIZE</code>=32 unless otherwise stated.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>To be fair, this could be much faster than it is currently. <a href=https://twitter.com/chhillee>Horace He</a> recommended that batching over heads would yield significant improvements, because there would be <code>N_HEADS</code> fewer kernel launches. I&rsquo;m not doing this to be practical though &ndash; I&rsquo;m doing it to be OPTIMAL&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Given this, you might reasonably ask why I would ever write the Torch version in the first place: because it&rsquo;s much easier to debug and understand. All other versions of this function are tested against the Torch version to ensure they return similar results.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://sophiawisdom.github.io/>&copy; Sophia Wisdom's Blog 2023</a><div><div class=ananke-socials></div></div></div></footer></body></html>