<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>New SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel | Sophia Wisdom's Blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="A friend of mine was desperate &ndash; just desperate &ndash; for speedy kernels. She knew the math, she knew the ML, she knew what had to be done. But her kernels were ludicrously slow because she didn&rsquo;t understand the GPU. The GPU is a fundamentally different machine than the CP, with fundamentally different capabilities. It is difficult to predict and understand what math operations can be made to run quickly on the GPU and why."><meta name=generator content="Hugo 0.110.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="New SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel"><meta property="og:description" content="A friend of mine was desperate &ndash; just desperate &ndash; for speedy kernels. She knew the math, she knew the ML, she knew what had to be done. But her kernels were ludicrously slow because she didn&rsquo;t understand the GPU. The GPU is a fundamentally different machine than the CP, with fundamentally different capabilities. It is difficult to predict and understand what math operations can be made to run quickly on the GPU and why."><meta property="og:type" content="article"><meta property="og:url" content="https://sophiawisdom.github.io/posts/ssm/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-17T04:20:46-08:00"><meta property="article:modified_time" content="2023-01-17T04:20:46-08:00"><meta itemprop=name content="New SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel"><meta itemprop=description content="A friend of mine was desperate &ndash; just desperate &ndash; for speedy kernels. She knew the math, she knew the ML, she knew what had to be done. But her kernels were ludicrously slow because she didn&rsquo;t understand the GPU. The GPU is a fundamentally different machine than the CP, with fundamentally different capabilities. It is difficult to predict and understand what math operations can be made to run quickly on the GPU and why."><meta itemprop=datePublished content="2023-01-17T04:20:46-08:00"><meta itemprop=dateModified content="2023-01-17T04:20:46-08:00"><meta itemprop=wordCount content="495"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="New SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel"><meta name=twitter:description content="A friend of mine was desperate &ndash; just desperate &ndash; for speedy kernels. She knew the math, she knew the ML, she knew what had to be done. But her kernels were ludicrously slow because she didn&rsquo;t understand the GPU. The GPU is a fundamentally different machine than the CP, with fundamentally different capabilities. It is difficult to predict and understand what math operations can be made to run quickly on the GPU and why."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Sophia Wisdom's Blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">New SSM Optimization: An introduction to GPU performance optimization through the lens of a single kernel</h1><time class="f6 mv4 dib tracked" datetime=2023-01-17T04:20:46-08:00>January 17, 2023</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>A friend of mine was desperate &ndash; just desperate &ndash; for speedy kernels. She knew the math, she knew the ML, she knew what had to be done. But her kernels were ludicrously slow because she didn&rsquo;t understand the GPU. The GPU is a fundamentally different machine than the CP, with fundamentally different capabilities. It is difficult to predict and understand what math operations can be made to run quickly on the GPU and why. This post is going to be a tour through optimizing several versions of a single kernel along with explanation of the hardware motivated by speed problems in the software.</p><p>But first: some background.</p><h1 id=why-are-gpus-such-a-big-deal-anyway>Why are GPUs such a big deal anyway?</h1><p>GPUs are accelerators. Their purpose is to make certain computations faster. They do not make most code faster. The vast majority of programs are written for CPUs, and programs written for GPUs are the hottest code imaginable. If you compare kernels written for GPUs to, say, React frontend code, you write ~100x less code and spend ~10000x as much energy running that code. Production Transformer models are a few thousand lines of CUDA (GPU C) that run for <em>millions</em> of GPU-hours. When you&rsquo;ve been reduced to the point of writing a GPU kernel, you have a desparate need for performance that cannot be slaked by writing the equivalent of Python, so even &ldquo;<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cuda-a-general-purpose-parallel-computing-platform-and-programming-model#high-level%20programming%20language:~:text=C%2B%2B%20as%20a-,high%2Dlevel%20programming%20language,-.%20As%20illustrated%20by">high level</a>&rdquo; programming languages require a solid understanding of the hardware to achieve acceptable performance.</p><p>What do GPUs accelerate? Originally graphics (<em>Graphics</em> Processing Unit) but now just any highly parallel code. There are a few tricks they use to be better at this than CPUs. The main one is that they do away with a lot of features that are not directly related to floating point computation: less <a href=https://en.wikipedia.org/wiki/CPU_cache>cache</a>, less <a href=https://en.wikipedia.org/wiki/Out-of-order_execution>OOO</a>, no <a href=https://en.wikipedia.org/wiki/Register_renaming>register renaming</a>, etc. Instead, they load up on tons of floating point execution units. These features are valuable, and sequential code executes less quickly for their absence. What do they get for this sacrifice? Well the AMD EPYC 9654 has the same number of transistors as the RTX 4090<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> and costs about ten times as much money<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> but the RTX 4090 can do ~80 float32 TFLOP/s where the EPYC 9654 can do ~10 float32 TFLOP/s. 8x better performance/transistor &ndash; hey that&rsquo;s not too bad!</p><p>Because they sacrified all those nice features, it&rsquo;s tricky to use the hardware efficiently.</p><p>The A100 is divided into many different Streaming Multiprocessors (SMs), each with their own local fast memory called &ldquo;shared memory&rdquo; and four Streaming Multiprocessor SubProcessors (SMSPs) which can be thought of for our purposes as 1024 bit wide vector cores (i.e. they do 32 32-bit floating point operations at a time instead of 1 at a time) and their own registers. They are <a href=https://en.wikipedia.org/wiki/Barrel_processor>barrel processors</a>, which can be thought of as Hyperthreading with up to 64 threads.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>78 billion for AMD EPYC 9654, 76 billion for RTX 4090.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>$11,000 for AMD EPYC 9654, $1,500 for RTX 4090.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://sophiawisdom.github.io/>&copy; Sophia Wisdom's Blog 2023</a><div><div class=ananke-socials></div></div></div></footer></body></html>